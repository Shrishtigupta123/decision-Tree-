{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL**\n",
        "\n",
        "\n",
        "1 What is a Decision Tree, and how does it work?\n",
        "-A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It works by splitting data into subsets based on feature values, forming a tree-like structure of decisions.\n",
        "\n",
        "How It Works:\n",
        "Root Node: The tree starts with a root node, which represents the entire dataset.\n",
        "\n",
        "Splitting: The dataset is split into branches based on the best feature that maximizes information gain (for classification) or minimizes variance (for regression).\n",
        "\n",
        "Decision Nodes: Each split creates internal nodes that further divide the data based on certain conditions.\n",
        "\n",
        "Leaf Nodes: When no further splits are possible, the branches end in leaf nodes, which represent the final classification or predicted value.\n",
        "\n",
        "Prediction: New data points follow the decision path in the tree until they reach a leaf node, which gives the final output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2 What are impurity measures in Decision Trees?\n",
        "-### **Impurity Measures in Decision Trees**\n",
        "Impurity measures determine how mixed or impure a dataset is at a given node. A Decision Tree aims to split data in a way that reduces impurity, making each resulting subset more homogeneous.\n",
        "\n",
        "### **1. Gini Impurity**\n",
        "- Measures how often a randomly chosen element would be incorrectly classified if randomly labeled.\n",
        "- Formula:  \n",
        "  \\[\n",
        "  Gini = 1 - \\sum_{i=1}^{n} p_i^2\n",
        "  \\]\n",
        "  where \\( p_i \\) is the proportion of class \\( i \\) in the node.\n",
        "- **Range:** [0, 0.5] (0 = pure node, 0.5 = maximum impurity with two equally probable classes)\n",
        "- Used in **CART (Classification and Regression Trees).**\n",
        "\n",
        "âœ… **Lower Gini = Better Split**  \n",
        "\n",
        "### **2. Entropy (Information Gain)**\n",
        "- Measures disorder (uncertainty) in the dataset.\n",
        "- Formula:  \n",
        "  \\[\n",
        "  Entropy = - \\sum_{i=1}^{n} p_i \\log_2 p_i\n",
        "  \\]\n",
        "- **Range:** [0, 1] (0 = pure node, 1 = maximum impurity for two classes)\n",
        "- Decision Trees aim to maximize **Information Gain**, which is:  \n",
        "  \\[\n",
        "  IG = Entropy_{parent} - \\sum \\left( \\frac{N_{child}}{N_{parent}} \\times Entropy_{child} \\right)\n",
        "  \\]\n",
        "\n",
        "âœ… **Higher Information Gain = Better Split**\n",
        "\n",
        "### **3. Variance Reduction (for Regression)**\n",
        "- Measures spread of continuous values in nodes.\n",
        "- Formula (for a node split at feature \\( X \\)):  \n",
        "  \\[\n",
        "  Variance = \\frac{1}{n} \\sum (y_i - \\bar{y})^2\n",
        "  \\]\n",
        "  where \\( y_i \\) are the target values, and \\( \\bar{y} \\) is the mean.\n",
        "- A split that **reduces variance** is considered good.\n",
        "\n",
        "âœ… **Lower Variance = Better Split**\n",
        "\n",
        "---\n",
        "### **Comparison of Gini vs. Entropy**\n",
        "| Criteria           | Gini Impurity | Entropy |\n",
        "|-------------------|--------------|---------|\n",
        "| Computation      | Faster       | Slower (log calculations) |\n",
        "| Preference      | Works well for most cases | Preferred if probability distribution is needed |\n",
        "| Values         | 0 (pure) to 0.5 (max) | 0 (pure) to 1 (max) |\n",
        "\n",
        "ðŸ“Œ **Gini is preferred in CART**, while **Entropy is used in ID3 & C4.5 Decision Trees.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3 What is the mathematical formula for Gini Impurity?\n",
        "-The **Gini Impurity** measures how often a randomly chosen element from a set would be incorrectly classified if randomly labeled based on the class distribution.\n",
        "\n",
        "### **Mathematical Formula for Gini Impurity**\n",
        "\\[\n",
        "Gini = 1 - \\sum_{i=1}^{n} p_i^2\n",
        "\\]\n",
        "where:\n",
        "- \\( n \\) = number of classes,\n",
        "- \\( p_i \\) = proportion (probability) of class \\( i \\) in the node.\n",
        "\n",
        "\n",
        "\n",
        "4 What is the mathematical formula for Entropy?\n",
        "-The **Entropy** measure in Decision Trees quantifies the uncertainty or impurity of a dataset. A higher entropy means more disorder, while lower entropy indicates more homogeneity.\n",
        "\n",
        "### **Mathematical Formula for Entropy**\n",
        "\\[\n",
        "Entropy = - \\sum_{i=1}^{n} p_i \\log_2 p_i\n",
        "\\]\n",
        "where:\n",
        "- \\( n \\) = number of classes,\n",
        "- \\( p_i \\) = proportion (probability) of class \\( i \\) in the node.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5 What is Information Gain, and how is it used in Decision Trees?\n",
        "-### **What is Information Gain?**\n",
        "**Information Gain (IG)** is a metric used in Decision Trees to determine which feature provides the best split at each node. It measures the reduction in uncertainty (or entropy) after splitting the dataset based on a particular feature.\n",
        "\n",
        "### **Mathematical Formula for Information Gain**\n",
        "\\[\n",
        "IG = Entropy_{parent} - \\sum_{i=1}^{k} \\frac{N_i}{N} \\times Entropy_i\n",
        "\\]\n",
        "where:\n",
        "- \\( Entropy_{parent} \\) = Entropy before splitting,\n",
        "- \\( k \\) = Number of child nodes after the split,\n",
        "- \\( N_i \\) = Number of samples in child node \\( i \\),\n",
        "- \\( N \\) = Total number of samples before the split,\n",
        "- \\( Entropy_i \\) = Entropy of child node \\( i \\).\n",
        "\n",
        "### **How Information Gain is Used in Decision Trees**\n",
        "1. **Calculate Entropy of the Parent Node:** Measure the impurity before splitting.\n",
        "2. **Split the Data Based on a Feature:** Divide the dataset based on possible values of a feature.\n",
        "3. **Calculate Weighted Entropy of Child Nodes:** Compute the entropy of each subset and weight it by the proportion of data in that subset.\n",
        "4. **Compute Information Gain:** Subtract the weighted entropy of children from the parent's entropy.\n",
        "5. **Select the Best Feature:** The feature with the **highest Information Gain** is chosen for the split.\n",
        "\n",
        "### **Example Calculation**\n",
        "Suppose we have a dataset where:\n",
        "- Before the split: **Entropy = 0.971**\n",
        "- After splitting into two child nodes:\n",
        "  - Node 1: \\( Entropy = 0.811 \\) (60% of data)\n",
        "  - Node 2: \\( Entropy = 0.918 \\) (40% of data)\n",
        "\n",
        "Using the formula:\n",
        "\\[\n",
        "IG = 0.971 - \\left( (0.6 \\times 0.811) + (0.4 \\times 0.918) \\right)\n",
        "\\]\n",
        "\\[\n",
        "IG = 0.971 - (0.4866 + 0.3672) = 0.971 - 0.8538 = 0.1172\n",
        "\\]\n",
        "\n",
        "Since Information Gain is low in this example, the feature may not be the best choice for splitting.\n",
        "\n",
        "ðŸ“Œ **Key Takeaways:**\n",
        "- **Higher IG** means a better feature for splitting.\n",
        "- Decision Trees aim to maximize **Information Gain** at each step.\n",
        "- **Entropy-based Information Gain** is commonly used in ID3, C4.5, and C5.0 algorithms.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6 What is the difference between Gini Impurity and Entropy?\n",
        "-### **Gini Impurity vs. Entropy: Key Differences**  \n",
        "\n",
        "Both **Gini Impurity** and **Entropy** are impurity measures used in Decision Trees to determine the best feature for splitting. However, they differ in calculation, interpretation, and performance.\n",
        "\n",
        "| Criteria         | **Gini Impurity** | **Entropy** |\n",
        "|-----------------|------------------|------------|\n",
        "| **Definition**  | Measures the probability of misclassification if a random sample is classified based on distribution. | Measures the disorder or uncertainty in a dataset. |\n",
        "| **Formula**     | \\[ Gini = 1 - \\sum p_i^2 \\] | \\[ Entropy = - \\sum p_i \\log_2 p_i \\] |\n",
        "| **Range**      | \\([0, 0.5]\\) for binary classification | \\([0, 1]\\) for binary classification |\n",
        "| **Computational Complexity** | Faster (does not use logarithms) | Slower (involves logarithmic calculations) |\n",
        "| **Preference**  | Used in **CART** (Classification and Regression Trees) | Used in **ID3, C4.5, and C5.0** decision trees |\n",
        "| **Behavior**    | Prefers **pure splits** | More sensitive to changes in probability distribution |\n",
        "| **Decision Boundary** | Slightly different, but mostly leads to the same splits as Entropy | Sometimes results in different splits due to logarithmic weighting |\n",
        "\n",
        "### **Which One to Use?**\n",
        "- **Gini Impurity** is computationally faster and often preferred when performance matters.\n",
        "- **Entropy (Information Gain)** can be more informative when class distributions vary significantly.\n",
        "\n",
        "\n",
        "\n",
        "7 What is the mathematical explanation behind Decision Trees?\n",
        "-### **Mathematical Explanation Behind Decision Trees**\n",
        "\n",
        "A **Decision Tree** is a supervised learning algorithm used for classification and regression tasks. It works by recursively partitioning the dataset into subsets based on feature values to minimize impurity.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Mathematical Foundation**\n",
        "A Decision Tree splits data at each node based on a selected feature \\( X_i \\) to maximize **Information Gain** or minimize impurity.\n",
        "\n",
        "### **1.1 Splitting Criteria**\n",
        "At each step, the algorithm chooses a feature that results in the most significant reduction in impurity.\n",
        "\n",
        "### **1.2 Impurity Measures**\n",
        "Impurity is a measure of how mixed the data is in a node. The goal is to reduce impurity after each split.\n",
        "\n",
        "#### **(a) Gini Impurity (Used in CART)**\n",
        "\\[\n",
        "Gini = 1 - \\sum_{i=1}^{n} p_i^2\n",
        "\\]\n",
        "where \\( p_i \\) is the probability of class \\( i \\) in the node.\n",
        "\n",
        "#### **(b) Entropy (Used in ID3, C4.5)**\n",
        "\\[\n",
        "Entropy = - \\sum_{i=1}^{n} p_i \\log_2 p_i\n",
        "\\]\n",
        "Entropy measures disorder. Lower entropy means purer nodes.\n",
        "\n",
        "#### **(c) Variance Reduction (Used for Regression Trees)**\n",
        "For continuous target variables, Decision Trees use **variance** instead of classification-based measures:\n",
        "\\[\n",
        "Variance = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
        "\\]\n",
        "where \\( y_i \\) is the target value, and \\( \\bar{y} \\) is the mean.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Information Gain (IG)**\n",
        "Decision Trees maximize **Information Gain**, which measures impurity reduction.\n",
        "\n",
        "\\[\n",
        "IG = Entropy_{parent} - \\sum_{i=1}^{k} \\frac{N_i}{N} \\times Entropy_i\n",
        "\\]\n",
        "where:\n",
        "- \\( N \\) = total number of samples in the parent node,\n",
        "- \\( N_i \\) = number of samples in the \\( i^{th} \\) child node,\n",
        "- \\( k \\) = number of child nodes.\n",
        "\n",
        "A feature with the **highest Information Gain** is chosen for the split.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Splitting Process**\n",
        "1. **Calculate impurity** (Gini, Entropy, or Variance) at the root node.\n",
        "2. **Find the best feature to split** (one that maximizes Information Gain or minimizes impurity).\n",
        "3. **Split the dataset** into child nodes.\n",
        "4. **Repeat recursively** until a stopping condition is met (e.g., max depth, minimum samples per leaf).\n",
        "5. **Assign class labels or values** at the leaf nodes.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Pruning (Overfitting Prevention)**\n",
        "### **(a) Pre-Pruning (Stopping Criteria)**\n",
        "- Limit **tree depth** (max depth).\n",
        "- Require a **minimum number of samples** to split.\n",
        "- Set a **minimum impurity decrease**.\n",
        "\n",
        "### **(b) Post-Pruning**\n",
        "- Grow the tree fully and then remove branches with low importance.\n",
        "- **Cost Complexity Pruning (CCP)** minimizes the trade-off between tree size and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Decision Boundary (Mathematical Intuition)**\n",
        "Decision Trees create **axis-aligned decision boundaries** in feature space.\n",
        "For a **binary classification problem**, a Decision Tree partitions the feature space into rectangular regions.\n",
        "\n",
        "- For **classification**, each region corresponds to a class label.\n",
        "- For **regression**, each region predicts an average value.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "ðŸ“Œ **Mathematically, a Decision Tree is a recursive partitioning algorithm that splits data based on impurity minimization.** It builds a tree structure where each split optimally reduces uncertainty.\n",
        "\n",
        "\n",
        "\n",
        "8 What is Pre-Pruning in Decision Trees?\n",
        "-Pre-Pruning is a technique used to prevent a Decision Tree from growing too large and overfitting the training data by stopping the tree-building process early. This is done by imposing constraints on the tree during its growth.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9  What is Post-Pruning in Decision Trees?\n",
        "-Post-Pruning (or Pruning after Growth) is a technique used to reduce overfitting by first growing a fully developed Decision Tree and then removing unnecessary branches. This helps simplify the tree while maintaining good performance on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "10  What is the difference between Pre-Pruning and Post-Pruning?\n",
        "-### **Difference Between Pre-Pruning and Post-Pruning in Decision Trees**  \n",
        "\n",
        "Pre-pruning and post-pruning are techniques used to **prevent overfitting** in Decision Trees by controlling tree growth. The key difference is **when the pruning happens**:  \n",
        "\n",
        "| **Aspect**       | **Pre-Pruning** | **Post-Pruning** |\n",
        "|-----------------|----------------|----------------|\n",
        "| **When Applied** | During tree growth | After full tree growth |\n",
        "| **How It Works** | Stops unnecessary splits early based on predefined conditions | Grows the full tree first, then removes unimportant branches |\n",
        "| **Computational Cost** | Lower (faster training) | Higher (must grow the full tree first) |\n",
        "| **Risk** | May **underfit** if the stopping criteria are too strict | More flexible but needs tuning |\n",
        "| **Stopping Criteria** | Max depth, min samples per split, min impurity decrease, etc. | Uses Cost Complexity Pruning (CCP) or validation set |\n",
        "| **Example Parameter (scikit-learn)** | `max_depth`, `min_samples_split`, `min_samples_leaf` | `ccp_alpha` (Cost Complexity Pruning) |\n",
        "| **Flexibility** | Less flexible (predefined limits may prevent useful splits) | More flexible (evaluates actual data performance before pruning) |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "11 What is a Decision Tree Regressor?\n",
        "-A Decision Tree Regressor is a type of Decision Tree used for regression tasks, where the goal is to predict a continuous numerical value instead of a class label.\n",
        "\n",
        "\n",
        "\n",
        "12 What are the advantages and disadvantages of Decision Trees?\n",
        "-### **Advantages of Decision Trees**\n",
        "\n",
        "1. **Interpretability and Simplicity**  \n",
        "   - **Easy to understand and interpret:** Decision Trees visually represent decisions, making them easy to interpret and explain, even for non-technical stakeholders.  \n",
        "   - **Clear decision-making process:** The tree structure makes it clear how decisions are made by the model.\n",
        "\n",
        "2. **Non-Linear Relationships**  \n",
        "   - **Captures non-linearity:** Decision Trees can model complex, non-linear relationships between features without needing transformation or feature engineering.\n",
        "\n",
        "3. **No Need for Feature Scaling**  \n",
        "   - **Works well without normalization or standardization:** Unlike algorithms like SVM or KNN, Decision Trees do not require features to be on the same scale, which simplifies data preprocessing.\n",
        "\n",
        "4. **Handles Both Numerical and Categorical Data**  \n",
        "   - Decision Trees can handle both **numerical** and **categorical** data, making them versatile.\n",
        "\n",
        "5. **Feature Selection**  \n",
        "   - Automatically performs **feature selection** by choosing the best features to split on. This makes them useful for high-dimensional datasets.\n",
        "\n",
        "6. **Robust to Outliers (in some cases)**  \n",
        "   - Decision Trees are somewhat **robust to outliers** because splits are made based on the feature values rather than the actual values themselves.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Decision Trees**\n",
        "\n",
        "1. **Prone to Overfitting**  \n",
        "   - **Overfitting:** Decision Trees are highly prone to overfitting, especially with deep trees that model noise in the data. This happens when the tree becomes too complex and perfectly fits the training data but fails to generalize to new data.\n",
        "   - **Mitigation:** Pre-pruning (limiting tree depth, minimum samples per leaf) or post-pruning techniques can help reduce overfitting.\n",
        "\n",
        "2. **Instability**  \n",
        "   - **Sensitive to data variations:** A small change in the data (like adding or removing a few points) can lead to a significantly different tree structure, making Decision Trees **unstable**.\n",
        "   - **Mitigation:** Random Forests (an ensemble method) can mitigate this by averaging multiple decision trees, thus improving stability.\n",
        "\n",
        "3. **Bias Toward Features with More Levels**  \n",
        "   - **Bias toward categorical features with many levels:** Decision Trees can favor features with more possible values or categories because they can split on many unique values, leading to overcomplicated models.\n",
        "\n",
        "4. **Can Create Unbalanced Trees**  \n",
        "   - **Unbalanced splits:** In cases where the data has imbalanced classes or extreme feature values, Decision Trees can create skewed or unbalanced splits, which negatively impacts model performance.\n",
        "   - **Mitigation:** Techniques like **cost-sensitive learning** or **class weights** can help when working with imbalanced data.\n",
        "\n",
        "5. **Poor Performance with Linear Models**  \n",
        "   - Decision Trees can perform poorly when data has **strong linear relationships**, as they tend to approximate the relationship as piecewise constant functions, which may not be ideal for continuous predictions.\n",
        "\n",
        "6. **Hard to Model Smooth Predictions (in Regression)**  \n",
        "   - In **regression**, Decision Trees predict constant values in each leaf. This can make the output **step-like**, which is not smooth and may not capture continuous trends well.\n",
        "   - **Mitigation:** Using **Random Forests** or **Gradient Boosting Trees** can smooth out predictions by averaging or combining trees.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "13 How does a Decision Tree handle missing values?\n",
        "-### **How Decision Trees Handle Missing Values**\n",
        "\n",
        "Decision Trees, in their standard form, do not inherently handle missing values during the training process. However, different methods can be employed to manage missing values, depending on the algorithm or library used. Here's how Decision Trees typically handle missing values:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Ignoring Samples with Missing Values**\n",
        "The simplest approach is to **ignore samples with missing values** during the split process. This means:\n",
        "- When a Decision Tree is being trained, it will simply **exclude rows** where a feature is missing for the current split.\n",
        "- Only the rows with known values are considered for the split, and the tree proceeds with the available data.\n",
        "\n",
        "**Downside:**  \n",
        "- **Loss of data**: This approach can lead to a significant loss of data, especially if the missing values are prevalent.\n",
        "- **Bias**: If the missing data is not random (e.g., missing due to certain conditions), this can introduce bias.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Surrogate Splits (Alternative Splits)**\n",
        "Some Decision Tree algorithms, such as **CART** (Classification and Regression Trees) and **C4.5**, use **surrogate splits** to handle missing values.\n",
        "\n",
        "- **Surrogate splits** allow the tree to use an **alternative feature** to make the split if the primary feature for the split is missing for some samples.\n",
        "- The surrogate split is chosen based on how closely it mimics the decision the tree would have made with the primary split.\n",
        "\n",
        "**How it works:**\n",
        "- A Decision Tree chooses the primary split for the data based on the feature that reduces the variance (for regression) or improves the purity (for classification).\n",
        "- If a sample has a missing value for that feature, the tree looks for a **secondary feature** (the surrogate) that produces a similar decision.\n",
        "\n",
        "**Advantages:**\n",
        "- **Reduces data loss** by using alternative features when the primary feature is missing.\n",
        "- **Maintains tree structure** by not discarding the sample.\n",
        "\n",
        "**Downside:**\n",
        "- Not all Decision Tree implementations support surrogate splits (e.g., `scikit-learn`'s Decision Tree does not).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Imputation (Preprocessing Step)**\n",
        "Another common strategy is to **impute missing values** before training the tree. Imputation involves filling in the missing values with reasonable estimates.\n",
        "\n",
        "- **Mean/Median Imputation:** For numerical data, missing values are replaced with the **mean** or **median** of the feature.\n",
        "- **Mode Imputation:** For categorical data, missing values are replaced with the **mode** (most frequent value).\n",
        "- **KNN Imputation:** A more sophisticated method that uses the **k-nearest neighbors** algorithm to predict missing values based on similar samples.\n",
        "- **Regression Imputation:** Missing values are predicted using a **regression model** trained on the other available features.\n",
        "\n",
        "**Advantages:**\n",
        "- **No data is discarded** since missing values are filled before training.\n",
        "- Ensures that the Decision Tree has complete data for all samples.\n",
        "\n",
        "**Downside:**\n",
        "- **Risk of bias**: If the missing data is not missing at random, imputing values can introduce bias into the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Using Libraries with Built-in Missing Value Handling**\n",
        "Some modern libraries like **XGBoost** and **LightGBM** have built-in mechanisms for handling missing values directly during the training process.\n",
        "\n",
        "- **XGBoost:** Automatically handles missing values by treating them as a separate category. The algorithm learns the best way to handle missing data by treating missing values as a separate \"missing\" category or using them during the tree-building process.\n",
        "- **LightGBM:** Similarly, LightGBM also handles missing values by treating them as a special category. It optimizes the way missing data is handled in each split.\n",
        "\n",
        "**Advantages:**\n",
        "- **Automatic handling** of missing values without the need for preprocessing or imputation.\n",
        "- **Efficient and robust** handling of missing data in large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Dropping Samples or Features**\n",
        "Another approach is to **remove** rows or columns that have too many missing values:\n",
        "\n",
        "- **Dropping Samples:** If a sample has missing values for critical features, it might be excluded from the dataset entirely.\n",
        "- **Dropping Features:** If a feature has too many missing values, it might be removed from the dataset entirely to avoid affecting the model's performance.\n",
        "\n",
        "**Advantages:**\n",
        "- Simple and straightforward approach when missing values are minimal.\n",
        "\n",
        "**Downside:**\n",
        "- **Data loss**: This approach can lead to a significant reduction in the dataset, especially if many samples or features have missing values.\n",
        "- **Bias**: Dropping data can introduce bias if missingness is related to a certain condition.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "14  How does a Decision Tree handle categorical features?\n",
        "-### **How Decision Trees Handle Categorical Features**\n",
        "\n",
        "Decision Trees are flexible enough to handle **categorical features** (features with discrete values or labels, such as \"Red\", \"Blue\", \"Green\") effectively. The way a Decision Tree handles categorical features differs depending on the algorithm or implementation, but the general process can be broken down into a few steps.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Splitting Categorical Features**\n",
        "\n",
        "When building a Decision Tree, the goal is to find the best feature and split point to divide the data into homogeneous subsets. For categorical features, the algorithm needs to decide how to best perform these splits.\n",
        "\n",
        "#### **For Binary Categorical Features (Two Categories)**\n",
        "- If a categorical feature has two possible categories (e.g., \"Yes\"/\"No\", \"True\"/\"False\"), the split is straightforward.\n",
        "- The tree simply creates a **binary split**:\n",
        "  - One branch for one category (e.g., \"Yes\").\n",
        "  - Another branch for the other category (e.g., \"No\").\n",
        "\n",
        "#### **For Multi-Class Categorical Features (More than Two Categories)**\n",
        "- If a categorical feature has multiple categories (e.g., \"Red\", \"Green\", \"Blue\"), the Decision Tree needs to decide how to split the data effectively.\n",
        "- **One-Hot Encoding** (or creating dummy variables) is sometimes used in some tree implementations like **scikit-learn** to handle these cases. Each category is treated as a separate binary feature, and the tree will decide which category (or set of categories) to split on.\n",
        "- **Alternative:** Some Decision Tree algorithms (e.g., **C4.5** and **ID3**) handle multi-class categorical features directly by choosing the best split using some form of **information gain** or **impurity reduction** (like Gini Impurity or Entropy).\n",
        "\n",
        "#### **Example Split for Multi-Class:**\n",
        "- If a feature \"Color\" has values \"Red\", \"Green\", \"Blue\", a Decision Tree might split the data like this:\n",
        "  - **Branch 1**: Samples where the color is \"Red\".\n",
        "  - **Branch 2**: Samples where the color is either \"Green\" or \"Blue\" (or even further sub-divided based on the remaining values).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Handling Categorical Features in Scikit-Learn**\n",
        "In **scikit-learn**, Decision Trees (like `DecisionTreeClassifier` and `DecisionTreeRegressor`) handle categorical features in the following way:\n",
        "\n",
        "- **Numerical representation:** If categorical features are **encoded numerically** (e.g., \"Red\" -> 0, \"Green\" -> 1, \"Blue\" -> 2), Decision Trees can handle them like continuous features. However, the tree will need to split based on numeric thresholds (e.g., `Feature = 1` for \"Green\").\n",
        "  \n",
        "- **One-Hot Encoding:** If categorical features are **one-hot encoded** (where each category is represented by a binary column), the tree will treat them as individual binary features and choose splits that maximize the separation between the categories.\n",
        "\n",
        "- **Handling multi-class:** In cases where a categorical feature has more than two categories, the tree can handle them by selecting the best way to partition the categories, either by:\n",
        "  - Treating each category as an individual binary feature (e.g., \"Is Red? Yes/No\", \"Is Green? Yes/No\").\n",
        "  - Splitting on combinations of categories if required.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Splitting Criteria for Categorical Features**\n",
        "Just like with numerical features, when splitting a categorical feature, the Decision Tree uses a **criterion** to determine the best way to partition the data. The most common criteria are:\n",
        "\n",
        "#### **Gini Impurity** (for classification)\n",
        "For a categorical target variable (classification problem), the algorithm chooses the categorical split that reduces the **impurity** the most.\n",
        "\n",
        "\\[\n",
        "Gini = 1 - \\sum p_i^2\n",
        "\\]\n",
        "\n",
        "Where \\(p_i\\) is the probability of class \\(i\\) at a given node.\n",
        "\n",
        "#### **Information Gain** (for classification)\n",
        "In algorithms like **ID3**, the decision tree splits based on **information gain** or **entropy**. Information gain measures how well a feature splits the data based on **entropy reduction**.\n",
        "\n",
        "\\[\n",
        "\\text{Information Gain} = \\text{Entropy(parent)} - \\sum \\left( \\frac{N_{\\text{child}}}{N_{\\text{parent}}} \\times \\text{Entropy(child)} \\right)\n",
        "\\]\n",
        "\n",
        "Where \\(N_{\\text{child}}\\) is the number of samples in a child node, and \\(N_{\\text{parent}}\\) is the number of samples in the parent node.\n",
        "\n",
        "#### **Chi-Squared or Other Statistical Tests**\n",
        "Some Decision Tree implementations (like **C4.5**) use statistical tests like the **Chi-squared test** to decide how to best split the categories based on the distribution of the target class.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Handling Rare Categories in Categorical Features**\n",
        "Rare or infrequent categories can pose problems in Decision Trees. Hereâ€™s how trees generally handle such categories:\n",
        "\n",
        "- **Infrequent Categories:** If a category is rare, the Decision Tree might overfit that category by making a decision based on too few samples. This might cause the model to generalize poorly.\n",
        "  - **Solution:** Pruning techniques can help remove overly specific branches.\n",
        "  - **Another solution:** Preprocessing methods like **combining rare categories** or **grouping similar categories** can help mitigate this issue.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Advantages and Disadvantages of Using Categorical Features in Decision Trees**\n",
        "| **Advantages**                                           | **Disadvantages**                                              |\n",
        "|----------------------------------------------------------|---------------------------------------------------------------|\n",
        "| **No need for feature scaling** â€“ Decision Trees do not require scaling for categorical features. | **Overfitting** â€“ Trees can overfit if too many categorical values create unnecessary splits. |\n",
        "| **Handles both numeric and categorical data** â€“ Decision Trees can directly handle categorical features. | **Complexity** â€“ The number of splits can grow quickly for high-cardinality categorical features. |\n",
        "| **Interpretability** â€“ Easy to understand how the categorical values lead to the decision. | **Rare categories** â€“ Can overfit rare categories or fail to generalize well with sparse data. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary: How Decision Trees Handle Categorical Features**\n",
        "- **For binary categorical features** (2 categories), Decision Trees simply split based on the two categories.\n",
        "- **For multi-class categorical features** (more than 2 categories), trees may use:\n",
        "  - **One-hot encoding** (splitting on multiple binary features).\n",
        "  - **Direct splits** based on the best separation.\n",
        "- **Splitting criteria** like **Gini Impurity** or **Information Gain** are used to decide how to split the categorical feature.\n",
        "- Libraries like **scikit-learn** handle categorical features either through direct encoding or by automatically processing them as binary features.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "15 What are some real-world applications of Decision Trees?\n",
        "-Decision Trees are widely used in various real-world applications due to their simplicity, interpretability, and ability to handle both classification and regression tasks. Here are some of the common real-world applications of Decision Trees:\n",
        "\n",
        "### **1. Medical Diagnosis**\n",
        "- **Purpose**: Decision Trees are used to diagnose diseases based on medical data.\n",
        "- **Example**: A Decision Tree can be trained to classify whether a patient has a certain disease based on symptoms, lab results, or medical history.\n",
        "- **Application**: Predicting whether a patient has cancer based on factors like age, blood pressure, genetic information, etc.\n",
        "\n",
        "### **2. Customer Segmentation in Marketing**\n",
        "- **Purpose**: Businesses use Decision Trees to segment their customers into groups based on behavior or demographics.\n",
        "- **Example**: A marketing team may use a Decision Tree to determine which customers are likely to respond to a specific campaign based on factors like purchase history, location, and social media activity.\n",
        "- **Application**: Identifying high-value customers for targeted marketing campaigns.\n",
        "\n",
        "### **3. Credit Scoring and Risk Assessment**\n",
        "- **Purpose**: Financial institutions use Decision Trees to assess the creditworthiness of loan applicants.\n",
        "- **Example**: The tree can classify applicants as low, medium, or high risk based on features like income, credit history, and employment status.\n",
        "- **Application**: Approving or denying loans or determining the interest rate based on risk level.\n",
        "\n",
        "### **4. Fraud Detection**\n",
        "- **Purpose**: Detecting fraudulent activities in transactions by identifying patterns of normal versus suspicious behavior.\n",
        "- **Example**: Credit card companies use Decision Trees to flag transactions that deviate from normal purchasing patterns (e.g., sudden large purchases from a different country).\n",
        "- **Application**: Detecting fraudulent transactions and preventing financial losses.\n",
        "\n",
        "### **5. Predicting Customer Churn**\n",
        "- **Purpose**: Companies in telecom, insurance, and subscription services use Decision Trees to predict which customers are likely to cancel or leave their service (churn).\n",
        "- **Example**: A telecom company may predict churn based on usage patterns, customer complaints, and payment behavior.\n",
        "- **Application**: Retaining customers by targeting those most likely to churn with special offers.\n",
        "\n",
        "### **6. Employee Attrition and Retention**\n",
        "- **Purpose**: Human resources departments use Decision Trees to predict which employees are at risk of leaving.\n",
        "- **Example**: A company may use employee data (e.g., job satisfaction, tenure, compensation, performance) to predict employee turnover.\n",
        "- **Application**: Retaining employees by identifying key factors that contribute to attrition and intervening early.\n",
        "\n",
        "### **7. Manufacturing and Quality Control**\n",
        "- **Purpose**: Decision Trees can help in quality control by classifying products based on quality checks and process conditions.\n",
        "- **Example**: A manufacturing company might use a Decision Tree to predict product defects based on machine settings, raw material quality, or temperature.\n",
        "- **Application**: Ensuring quality standards by identifying defective products early in the production process.\n",
        "\n",
        "### **8. Supply Chain Optimization**\n",
        "- **Purpose**: Decision Trees help in optimizing supply chain decisions, such as inventory management or demand forecasting.\n",
        "- **Example**: A retail company might use Decision Trees to forecast demand for products based on historical sales data, seasonal trends, and market conditions.\n",
        "- **Application**: Ensuring products are in stock when needed, without overstocking.\n",
        "\n",
        "### **9. Political Analysis and Voting Prediction**\n",
        "- **Purpose**: Predicting election outcomes or analyzing public opinion.\n",
        "- **Example**: Decision Trees can be used to predict election results based on factors like demographics, historical voting patterns, and economic indicators.\n",
        "- **Application**: Assisting political campaigns by analyzing voter preferences and behavior.\n",
        "\n",
        "### **10. Image Classification and Computer Vision**\n",
        "- **Purpose**: Decision Trees can be used for image classification tasks, where each pixel or feature represents a decision point.\n",
        "- **Example**: In medical imaging, Decision Trees might classify regions of an X-ray or MRI as either normal or indicative of a condition.\n",
        "- **Application**: Automating the detection of abnormalities in medical images.\n",
        "\n",
        "### **11. Environmental Science**\n",
        "- **Purpose**: Predicting environmental outcomes based on various factors like weather, pollution levels, and geographical conditions.\n",
        "- **Example**: Decision Trees can help predict air quality, soil erosion, or water quality based on historical environmental data.\n",
        "- **Application**: Helping policymakers make informed decisions to address environmental issues.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Decision Trees are versatile models that can be applied to many different fields. They are especially useful in domains where interpretability and transparency are crucial, such as healthcare, finance, and customer service. Their ability to handle both categorical and numerical data makes them applicable across a wide range of industries.\n",
        "\n"
      ],
      "metadata": {
        "id": "0DfBZFRavbHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "**PRACTICAL**\n",
        "\n",
        "\n",
        "16 Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "17 Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "feature importances\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini Impurity as the criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "18 Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Entropy as the criterion\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "19 Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data  # Features (input data)\n",
        "y = data.target  # Target (housing prices)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the regressor on the training data\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "20 Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.metrics import accuracy_score\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Visualize the trained Decision Tree\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create a Graphviz Source object and render it\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree_iris\", view=True)  # This will save and open the tree image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "21 Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with max_depth=3\n",
        "clf_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth_3.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set with max_depth=3\n",
        "y_pred_depth_3 = clf_depth_3.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for the tree with max_depth=3\n",
        "accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
        "\n",
        "# Initialize and train the fully grown Decision Tree Classifier (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set with the fully grown tree\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for the fully grown tree\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the comparison of accuracies\n",
        "print(f\"Accuracy of the Decision Tree with max depth 3: {accuracy_depth_3 * 100:.2f}%\")\n",
        "print(f\"Accuracy of the fully grown Decision Tree: {accuracy_full * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "22 Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_samples_split_5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_min_samples_split_5.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set with min_samples_split=5\n",
        "y_pred_min_samples_split_5 = clf_min_samples_split_5.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for the tree with min_samples_split=5\n",
        "accuracy_min_samples_split_5 = accuracy_score(y_test, y_pred_min_samples_split_5)\n",
        "\n",
        "# Initialize and train the default Decision Tree Classifier (no min_samples_split limit)\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set with the default tree\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for the default tree\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the comparison of accuracies\n",
        "print(f\"Accuracy of the Decision Tree with min_samples_split=5: {accuracy_min_samples_split_5 * 100:.2f}%\")\n",
        "print(f\"Accuracy of the default Decision Tree: {accuracy_default * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "23 Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Model with unscaled data ---\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the unscaled data\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set with unscaled data\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy for the unscaled data\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --- Model with scaled data ---\n",
        "# Apply Standard Scaling to the features (only scale the features, not the labels)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the scaled data\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set with scaled data\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy for the scaled data\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the comparison of accuracies\n",
        "print(f\"Accuracy of the Decision Tree with unscaled data: {accuracy_unscaled * 100:.2f}%\")\n",
        "print(f\"Accuracy of the Decision Tree with scaled data: {accuracy_scaled * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "24 Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize the One-vs-Rest strategy with Decision Tree Classifier as the base classifier\n",
        "ovr_classifier = OneVsRestClassifier(dt_classifier)\n",
        "\n",
        "# Train the classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy of the One-vs-Rest Decision Tree Classifier: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "25 Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels (target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Display the feature importance scores\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Print the feature importance scores with feature names\n",
        "feature_names = iris.feature_names\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the features by their importance in descending order\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the sorted feature importance\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(importance_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "26 Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Train the Decision Tree Regressor with max_depth=5 ---\n",
        "dt_regressor_depth_5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "dt_regressor_depth_5.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the model having max_depth=5\n",
        "y_pred_depth_5 = dt_regressor_depth_5.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error for the tree with max_depth=5\n",
        "mse_depth_5 = mean_squared_error(y_test, y_pred_depth_5)\n",
        "\n",
        "# --- Train the unrestricted Decision Tree Regressor ---\n",
        "dt_regressor_unrestricted = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor_unrestricted.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the unrestricted model\n",
        "y_pred_unrestricted = dt_regressor_unrestricted.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error for the unrestricted tree\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# Print the comparison of MSE values\n",
        "print(f\"Mean Squared Error of the Decision Tree with max_depth=5: {mse_depth_5:.4f}\")\n",
        "print(f\"Mean Squared Error of the unrestricted Decision Tree: {mse_unrestricted:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "27 Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy\n",
        "-# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier without pruning\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "initial_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Apply Cost Complexity Pruning (CCP)\n",
        "# Get the effective alphas and the corresponding tree sizes\n",
        "path = dt_classifier.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "tree_sizes = path.tree_['node_count']\n",
        "\n",
        "# Store accuracy scores for each alpha\n",
        "accuracies = []\n",
        "\n",
        "# Loop through each alpha to prune the tree and evaluate accuracy\n",
        "for alpha in ccp_alphas:\n",
        "    # Train the model with the current alpha (pruned tree)\n",
        "    pruned_dt_classifier = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    pruned_dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and calculate accuracy\n",
        "    y_pred_pruned = pruned_dt_classifier.predict(X_test)\n",
        "    accuracies.append(accuracy_score(y_test, y_pred_pruned))\n",
        "\n",
        "# Plot the accuracy vs. ccp_alpha\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, accuracies, marker='o', label=\"Accuracy vs. CCP Alpha\")\n",
        "plt.axhline(initial_accuracy, color=\"red\", linestyle=\"--\", label=\"Unpruned Tree Accuracy\")\n",
        "plt.xlabel(\"CCP Alpha\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning on Decision Tree Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print the final accuracy for the unpruned tree\n",
        "print(f\"Initial accuracy (unpruned tree): {initial_accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "28 Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "29 Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn\n",
        "-# Import necessary libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using Seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.title(\"Confusion Matrix for Decision Tree Classifier\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "30 Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split\n",
        "-# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],  # Test different depths\n",
        "    'min_samples_split': [2, 5, 10]  # Test different minimum samples to split a node\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt_classifier, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from the GridSearchCV\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train the Decision Tree Classifier with the optimal parameters\n",
        "optimal_dt_classifier = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = optimal_dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Best parameters found by GridSearchCV: {best_params}\")\n",
        "print(f\"Test set accuracy with optimal parameters: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "wlUd6eYG1l40"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}